{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:49:59.183719Z",
     "start_time": "2019-05-09T21:49:59.060803Z"
    }
   },
   "outputs": [],
   "source": [
    "# for deleting python script to easily resave with same name\n",
    "#!rm Mod1_Functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T02:30:11.224616Z",
     "start_time": "2019-05-10T02:30:10.326905Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFE\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T18:58:46.663855Z",
     "start_time": "2019-05-06T18:58:46.658742Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_datafield(data, col_name, convert_type=None, val_to_replace=None, val_replacement=None, replacement_array=None):\n",
    "    \"\"\"Takes inputs of data as pandas.dataframe and column name as string\n",
    "    Returns a pandas series of clean values\n",
    "    \"\"\"\n",
    "    outval = pd.Series()\n",
    "    if val_to_replace != None:\n",
    "        if type(val_replacement) == list:\n",
    "            # pass\n",
    "            outval = np.where(data[col_name] == val_to_replace, sum(\n",
    "                [data[val_replacement[i]]*replacement_array[i] for i in range(len(replacement_array))]), data[col_name])\n",
    "        else:\n",
    "            outval = data[col_name].copy().replace(\n",
    "                to_replace=val_to_replace, value=val_replacement)\n",
    "    else:\n",
    "        outval = data[col_name].copy()\n",
    "\n",
    "    if convert_type != None:\n",
    "        outval = outval.astype(convert_type)\n",
    "    return outval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T19:28:56.010747Z",
     "start_time": "2019-05-06T19:28:56.006299Z"
    }
   },
   "outputs": [],
   "source": [
    "# values of adjustment dictionary in format of [convert_type, val_to_replace, val_replacement]\n",
    "\n",
    "#data_adjustments = {'date': ['datetime64', None, None, None], \n",
    "#                    'bedrooms': [None, 33, 4, None], \n",
    "#                    'waterfront': [str, np.nan, 'missing', None], \n",
    "#                    'view': [str, np.nan, 0, None],\n",
    "#                    'sqft_basement': [float, '?', ['sqft_living','sqft_above'], [1, -1]]\n",
    "#                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T19:28:57.234722Z",
     "start_time": "2019-05-06T19:28:57.230185Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_dataframe(data, data_adj={}):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    out_df = pd.DataFrame()\n",
    "    for column_name in data.columns:\n",
    "        if column_name in data_adj.keys():\n",
    "            dict_look = data_adj[column_name]\n",
    "        else:\n",
    "            dict_look = [None, None, None, None]\n",
    "        out_df[column_name] = clean_datafield(\n",
    "            data, column_name, dict_look[0], dict_look[1], dict_look[2], dict_look[3])\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T20:38:40.321470Z",
     "start_time": "2019-05-06T20:38:40.314731Z"
    }
   },
   "outputs": [],
   "source": [
    "def renovated_cat(series, n_years):\n",
    "    if series == 0:\n",
    "        return 'Never Renovated'\n",
    "    elif (2015-n_years) <= series <= 2015:\n",
    "        return 'Since {} inclusive'.format(2015-n_years)\n",
    "    elif series < (2015-n_years):\n",
    "        return 'Prior to {}'.format(2015-n_years)\n",
    "    else:\n",
    "        return 'missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T20:19:22.361484Z",
     "start_time": "2019-05-06T20:19:22.358329Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_to_categorical(df, listofcolumns):\n",
    "    for col in listofcolumns:\n",
    "        df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T02:23:29.692609Z",
     "start_time": "2019-05-08T02:23:29.686735Z"
    }
   },
   "outputs": [],
   "source": [
    "def scatter_y(df, y, ncols=3, figsize=(16, 20), wspace=0.2, hspace=0.5, alpha=0.05, color='b'):\n",
    "    df_col_list = list(df.columns)\n",
    "    if (len(df_col_list) % ncols > 0):\n",
    "        nrows = len(df_col_list)//ncols + 1\n",
    "    else:\n",
    "        nrows = len(df_col_list)//ncols\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    fig.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "\n",
    "    for i, xcol in enumerate(df_col_list):\n",
    "        try:\n",
    "            df.plot(kind='scatter', x=xcol, y=y,\n",
    "                    ax=axes[i//ncols, i % ncols], label=xcol, alpha=alpha, color=color)\n",
    "            plt.plot()\n",
    "        except:\n",
    "            print('warning: could not graph {} as numbers'.format(xcol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T03:33:23.899674Z",
     "start_time": "2019-05-07T03:33:23.896098Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dummyframe(df, listofcolumns):\n",
    "    df_dummy = pd.DataFrame()\n",
    "    for cat_col in listofcolumns:\n",
    "        df_dummy = pd.concat([df_dummy, pd.get_dummies(df[cat_col], prefix=cat_col)], axis=1)\n",
    "    return df_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T15:34:56.660794Z",
     "start_time": "2019-05-07T15:34:56.655584Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_season(month):\n",
    "    month = str(month)\n",
    "    if month in ['12', '1', '2']:\n",
    "        return 'Winter'\n",
    "    elif month in ['3', '4', '5']:\n",
    "        return 'Spring'\n",
    "    elif month in ['6', '7', '8']:\n",
    "        return 'Summer'\n",
    "    elif month in ['9', '10', '11']:\n",
    "        return 'Fall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findcorrpairs(df, corr_thresh=0.75, round=2):\n",
    "    corr_mx = df.corr()\n",
    "    pairs_list = []\n",
    "    corr_list = []\n",
    "    for col in corr_mx.columns:\n",
    "        for col2 in corr_mx.columns:\n",
    "            if col != col2:\n",
    "                pair_corr = corr_mx[col][col2]\n",
    "                if abs(pair_corr)>corr_thresh:\n",
    "                    col_sort = [col, col2]\n",
    "                    col_sort.sort()\n",
    "                    pairs_list.append(col_sort)\n",
    "                    corr_list.append(pair_corr.round(round))\n",
    "    df_out = pd.DataFrame()\n",
    "    df_out['Pairs'] = pairs_list\n",
    "    df_out['PairsText'] = df_out['Pairs'].astype('str')\n",
    "    df_out['Correlation'] = corr_list\n",
    "    df_out.drop_duplicates(subset='PairsText', inplace = True)\n",
    "    df_out.sort_values('Correlation', ascending=False, inplace=True)\n",
    "    df_out.drop('PairsText',  axis=1, inplace=True)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bathroom_bins(bathroom):\n",
    "    return math.ceil(bathroom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    # Radius of earth in kilometers is 6371\n",
    "    km = 6371* c\n",
    "    return km\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historic_home(age):\n",
    "    if age>=80:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T02:30:13.104401Z",
     "start_time": "2019-05-10T02:30:13.100984Z"
    }
   },
   "outputs": [],
   "source": [
    "df_out=pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T22:01:42.024272Z",
     "start_time": "2019-05-09T22:01:42.015057Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_features_to_df(df):\n",
    "    \"\"\"Output is a new dataframe with our additional features added\"\"\"\n",
    "    df_out = df.copy()\n",
    "    try:\n",
    "        df_out['month'] = df_out['date'].map(lambda x: x.month)\n",
    "        df_out['season'] = df_out['month'].apply(create_season)\n",
    "    except:\n",
    "        print('Error: date was not found in dataframe.  Month and season could not be created')\n",
    "\n",
    "    try:\n",
    "        df_out['has_basement'] = df_out['sqft_basement'].apply(\n",
    "            lambda x: 1 if x > 0 else 0)\n",
    "    except:\n",
    "        print('Error: sqft_basement was not found in dataframe.  has_basement could not be created')\n",
    "\n",
    "    try:\n",
    "        df_out['sqft_above_tophalf'] = df_out['sqft_above'].apply(\n",
    "            lambda x: 1 if x > df_out['sqft_above'].mean() else 0)\n",
    "    except:\n",
    "        print('Error: sqft_above was not found in dataframe.  sqft_above_tophalf could not be created')\n",
    "\n",
    "    try:\n",
    "        df_out['zip_highprice'] = df_out['zipcode'].apply(\n",
    "        lambda x: 1 if x in [98004, 98039, 98040, 98112] else 0)\n",
    "    except:\n",
    "        print('Error: zipcode was not found in dataframe.  zip_highprice could not be created')\n",
    "\n",
    "# Set number of years to consider recent renovation\n",
    "    try:\n",
    "        n_years = 15\n",
    "        df_out['yr_renovated_cat'] = df_out['yr_renovated'].apply(\n",
    "            renovated_cat, n_years=n_years)\n",
    "    except:\n",
    "        print('Error: yr_renovated was not found in dataframe.  yr_renovated_cat could not be created')\n",
    "\n",
    "    \n",
    "    try:\n",
    "        df_out['startdate'] = pd.Timestamp('19700101')\n",
    "        df_out['date_num'] = (df_out['date'] - df_out['startdate']).dt.days\n",
    "        df_out.drop(['startdate'], axis=1)\n",
    "    except:\n",
    "        print('Error: date was not found in dataframe.  date_num could not be created')\n",
    "\n",
    "    try:\n",
    "        # Calculated Distance from City Center\n",
    "        df_out['dist_city_center'] = df_out.apply(\n",
    "            lambda x: haversine(-122.3344, 47.6050, x.loc['long'], x.loc['lat']), axis=1)\n",
    "    except:\n",
    "        print('Error: long and/or lat was not found in dataframe.  dist_city_center could not be created')\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        df_out['bathroom_bins'] = df_out.bathrooms.apply(bathroom_bins)\n",
    "    except:\n",
    "        print('Error: bathrooms was not found in dataframe.  bathroom_bins could not be created')\n",
    "        \n",
    "    try:\n",
    "        df_out['age'] = df_out['date'].apply(lambda x: x.year) - df_out['yr_built']\n",
    "        df_out['historic_home'] = df_out.age.apply(historic_home)\n",
    "    except:\n",
    "        print('Error: date and/or yr_built was not found in dataframe.  age and historic_home could not be created') \n",
    "        \n",
    "    try:\n",
    "        df_out['grade_bin'] = df_out.grade.apply(grade_bins)\n",
    "    except:\n",
    "        print('Error: grade was not found in dataframe.  grade_bin could not be created') \n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T19:14:01.461539Z",
     "start_time": "2019-05-09T19:14:01.456709Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_df_quantiles(df, filter_dict):\n",
    "    df_filter = df.copy()\n",
    "\n",
    "    for k, v in filter_dict.items():\n",
    "        min_lim = df[k].quantile(v[0])\n",
    "        max_lim = df[k].quantile(v[1])\n",
    "\n",
    "        n_less = len(df_filter[df_filter[k] < min_lim])\n",
    "        n_greater = len(df_filter[df_filter[k] > max_lim])\n",
    "\n",
    "        filter1 = df_filter[k] >= min_lim\n",
    "        filter2 = df_filter[k] <= max_lim\n",
    "\n",
    "        print('filtered out {0} records with {1} less than: {2:.2f}'.format(n_less, k, min_lim))\n",
    "        print('filtered out {0} records with {1} greater than: {2:.2f}'.format(n_greater,  k, max_lim))\n",
    "\n",
    "        df_filter = df_filter[filter1 & filter2]\n",
    "\n",
    "    print('{} total records removed'.format(len(df) - len(df_filter)))\n",
    "        \n",
    "    return df_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T22:01:40.648777Z",
     "start_time": "2019-05-09T22:01:40.645831Z"
    }
   },
   "outputs": [],
   "source": [
    "def grade_bins(grade):\n",
    "    if grade >= 11:\n",
    "        return 11\n",
    "    elif grade <= 5:\n",
    "        return 5\n",
    "    else:\n",
    "        return grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T01:15:11.343807Z",
     "start_time": "2019-05-10T01:15:11.336249Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mse_train_test(x, y, start_test_pct=5, test_pct_inc=5, num_iter=50):   \n",
    "    linreg = LinearRegression()\n",
    "\n",
    "    mse_train_list = []\n",
    "    mse_test_list = []\n",
    "    s_list = []\n",
    "    \n",
    "    for s in range(start_test_pct, 100, test_pct_inc):\n",
    "        mse_train_sum = 0\n",
    "        mse_test_sum = 0\n",
    "        for i in range(num_iter):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = s/100)\n",
    "            linreg.fit(X_train, y_train)\n",
    "            y_hat_train = linreg.predict(X_train)\n",
    "            y_hat_test = linreg.predict(X_test)\n",
    "            train_residuals = y_hat_train - y_train\n",
    "            test_residuals = y_hat_test - y_test\n",
    "            mse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\n",
    "            mse_test =np.sum((y_test-y_hat_test)**2)/len(y_test)\n",
    "            mse_train_sum += mse_train\n",
    "            mse_test_sum += mse_test \n",
    "\n",
    "        mse_train_list.append(mse_train_sum / num_iter)\n",
    "        mse_test_list.append(mse_test_sum / num_iter)\n",
    "        s_list.append(s)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.scatterplot(s_list, mse_train_list, label=\"Training Error\")\n",
    "    sns.scatterplot(s_list, mse_test_list, label=\"Testing Error\")\n",
    "    plt.xlabel('Testing Data Size as Percentage of Total Dataset')\n",
    "    plt.ylabel('Mean Square Error (Average of {} trials)'.format(num_iter))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T01:22:28.646806Z",
     "start_time": "2019-05-10T01:22:28.643300Z"
    }
   },
   "outputs": [],
   "source": [
    "def residual_hist_qq(model):\n",
    "    resid1 = model.resid\n",
    "    resid1.hist()\n",
    "    fig = sm.graphics.qqplot(resid1, dist=stats.norm, line='45', fit=True)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T02:23:08.228121Z",
     "start_time": "2019-05-10T02:23:08.222127Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_RFE_var_iter(X, Y, k_fold_n_splits=5, shuffle=True, scoring='neg_mean_squared_error'):\n",
    "\n",
    "\n",
    "    n = len(X.columns)\n",
    "\n",
    "    mse_list = []\n",
    "    r2_list = []\n",
    "    num_var = list(range(1, n+1))\n",
    "\n",
    "    for i in range(n):\n",
    "        linreg = LinearRegression()\n",
    "        selector = RFE(linreg, n_features_to_select=i+1)\n",
    "        selector = selector.fit(X, Y.squeeze())\n",
    "        selected_X = X.columns[selector.support_]\n",
    "        df_selected = X.loc[:, selected_X]\n",
    "        df_selected = sm.add_constant(df_selected)\n",
    "        model_kfold = KFold(n_splits=k_fold_n_splits, shuffle=shuffle)\n",
    "        MSEs = cross_val_score(linreg, df_selected, Y,\n",
    "                               scoring=scoring, cv=model_kfold)\n",
    "        mean_MSE = -1 * np.mean(MSEs)\n",
    "        model = sm.OLS(Y, df_selected).fit()\n",
    "\n",
    "        r2_list.append(model.rsquared_adj)\n",
    "        mse_list.append(mean_MSE)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(r2_list, label='Adjusted $r^{2}$')\n",
    "    plt.xlabel(r\"Number of X Variables Included in Model\")\n",
    "    plt.ylabel(r\"Mean Squared Error / Adjusted r squared\")\n",
    "    plt.title(\"MSE and Adjusted $r^{2}$ versus Number of Model Variables\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.plot(mse_list, label='Mean Squared Error')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T04:40:18.342956Z",
     "start_time": "2019-05-10T04:40:18.332473Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df, categorical_columns=[], log_list=[],  min_max_list=[], std_scal_list=[], dropout_list=[]):\n",
    "    \"\"\"Custom preprocessing\"\"\"\n",
    "\n",
    "\n",
    "# create temporary data frame to work with\n",
    "    df_temp = df.copy()\n",
    "\n",
    "# handle categorical columns\n",
    "# set to categorical, then dummy out and store in df_dummy\n",
    "    if len(categorical_columns) > 0:\n",
    "        set_to_categorical(df_temp, categorical_columns)\n",
    "\n",
    "        print(\"Categorical Variables:\")\n",
    "        print(df_temp.dtypes[df_temp.dtypes == 'category'])\n",
    "        print(\"\\n\")\n",
    "\n",
    "        df_dummy = create_dummyframe(df_temp, categorical_columns)\n",
    "\n",
    "        drop_list = []\n",
    "\n",
    "        for drop_col in list(df_temp.dtypes[df_temp.dtypes == 'category'].keys()):\n",
    "            field_name = drop_col + '_' + \\\n",
    "                str(df_temp[drop_col].cat.categories[0])\n",
    "      #      print('dropped {} to avoid multicollinearity'.format(field_name))\n",
    "            drop_list.append(field_name)\n",
    "\n",
    "        df_dummy.drop(drop_list, axis=1, inplace=True)\n",
    "        print('To avoid multicollinearity, the following datafields were dropped: {}'.format(\", \".join(drop_list)))\n",
    "        df_temp.drop(categorical_columns, axis=1, inplace=True)\n",
    "    else:\n",
    "        print('no categorical dummy columns added')\n",
    "        df_dummy = pd.DataFrame()\n",
    "\n",
    "        \n",
    "\n",
    "#  natural log of variables, note return of 0 for non-positive values\n",
    "    if len(log_list) > 0:\n",
    "        print('\\n')\n",
    "        for i in log_list:\n",
    "            df_temp[i] = df_temp[i].apply(lambda x: np.log(x) if x > 0 else 0)\n",
    "        print('Converted the following datafields to natural log: {}'.format(\", \".join(log_list)))\n",
    "\n",
    "\n",
    "# Min Max scaling\n",
    "    print('\\n')\n",
    "    if len(min_max_list) > 0:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        X_min_max = pd.DataFrame(columns=min_max_list)\n",
    "        for col_name in min_max_list:\n",
    "            X_min_max[col_name] = df_temp[col_name]\n",
    "            print('Converted {0} to scale min-max: [{1:.2f}, {2:.2f}] to [0,1]'.format(\n",
    "                col_name, df_temp[col_name].min(), df_temp[col_name].max()))\n",
    "\n",
    "        X_min_max = pd.DataFrame(data=min_max_scaler.fit_transform(\n",
    "            X_min_max.values), columns=X_min_max.columns, index=X_min_max.index)\n",
    "        df_temp.drop(min_max_list, axis=1, inplace=True)\n",
    "    else:\n",
    "        X_min_max = pd.DataFrame()\n",
    "        print('No variables scaled with min-max scaler')\n",
    "\n",
    "\n",
    "# Standard scaling\n",
    "    print('\\n')\n",
    "    if len(std_scal_list) > 0:\n",
    "        standard_scaler = preprocessing.StandardScaler()\n",
    "        X_std_scal = pd.DataFrame(columns=std_scal_list)\n",
    "        for col_name in std_scal_list:\n",
    "            X_std_scal[col_name] = df_temp[col_name]\n",
    "            print('Standardized {0} to scale with mean=0 and std=1 from: [{1:.3f}, {2:.3f}] to [0,1]'.format(\n",
    "                col_name, df_temp[col_name].mean(), df_temp[col_name].std()))\n",
    "\n",
    "        X_std_scal = pd.DataFrame(data=standard_scaler.fit_transform(\n",
    "            X_std_scal.values), columns=X_std_scal.columns, index=X_min_max.index)\n",
    "        df_temp.drop(std_scal_list, axis=1, inplace=True)\n",
    "    else:\n",
    "        X_std_scal = pd.DataFrame()\n",
    "        print('No variables scaled with standard scaler')\n",
    "\n",
    "# drop any columns in the drop list\n",
    "    if len(dropout_list)>0:\n",
    "        print('\\n')\n",
    "        df_temp.drop(dropout_list, axis=1, inplace=True)\n",
    "        print(\"Dropped {} from the output dataset\".format(\", \".join(dropout_list)))\n",
    "        \n",
    "# concatenate all of these dataframes\n",
    "    X_possible = pd.concat([X_min_max, X_std_scal, df_temp, df_dummy], axis=1)\n",
    "    \n",
    "    return X_possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T05:45:05.205202Z",
     "start_time": "2019-05-10T05:45:05.198570Z"
    }
   },
   "outputs": [],
   "source": [
    "def interpret_coef(model, df_original, categorical_columns=[], log_list=[],  min_max_list=[], std_scal_list=[]):\n",
    "\n",
    "    var_coef = model.params[1:]\n",
    "    adj_coef = []\n",
    "    interpret = []\n",
    "\n",
    "    for col, coef in var_coef.items():\n",
    "        if col in min_max_list:\n",
    "            adj_coef.append((np.exp(coef)-1) * 100 /\n",
    "                            (df_original[col].max() - df_original[col].min()))\n",
    "\n",
    "        elif col in std_scal_list:\n",
    "            adj_coef.append((np.exp(coef)-1) * 100 / (df_original[col].std()))\n",
    "\n",
    "        elif col in log_list:\n",
    "            adj_coef.append(coef)\n",
    "        else:\n",
    "            adj_coef.append((np.exp(coef)-1) * 100)\n",
    "\n",
    "    for i, col in enumerate(var_coef.keys()):\n",
    "        if col in (min_max_list + std_scal_list):\n",
    "            interpret.append(\"Price is expected to increase by {0:.4f}% for each unit increment beyond {1:.2f} in {2}\".format(\n",
    "                adj_coef[i], df_original[col].min(), col))\n",
    "\n",
    "        elif col in log_list:\n",
    "            interpret.append(\n",
    "                \"Price is expected to increase by {0:.4f}% for each 1% increase in {1}\".format(adj_coef[i], col))\n",
    "        elif str(col).split('_')[0] in categorical_columns:\n",
    "            interpret.append(\n",
    "                \"Price is expected to increase by {0:.4f}% if condition met for {1}\".format(adj_coef[i], col))\n",
    "        else:\n",
    "            interpret.append(\"Price is expected to increase by {0:.4f}% for each unit increment in {1}\".format(\n",
    "                adj_coef[i], col))\n",
    "    return pd.DataFrame(list(zip(list(var_coef.keys()), list(var_coef.values), interpret)), columns=['variable', 'coef', 'interpretation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T05:51:06.509239Z",
     "start_time": "2019-05-10T05:51:06.505391Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
