{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-weight:bold; font-size:35px; line-height:1.0;\">EDA: Sell Your House for More </div>\n",
    "\n",
    "<figure style=\"margin-top:10px\"><img src=\"https://www.racialequityalliance.org/wp-content/uploads/2016/10/assessors_social-1.jpg\" />\n",
    "<figcaption style=\"text-align:center; font-size:8px\">source: https://www.racialequityalliance.org/jurisdictions/king-county-washington/assessors_social/</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Imports and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we get more money for our house in King County?\n",
    "\n",
    "We are selling our house and would like to know which characteristics of a home can help improve the sale price.\n",
    "\n",
    "In order to help answer this question, we have been provided a dataset of home sales in King County which occurred during the period of September 9, 2014 through January 10, 2015.\n",
    "\n",
    "We will attempt to model sale prices based on the other data fields and then determine which characteristics lead to an increase in sale price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was provided in a csv format.  There are 21,597 sales records and 21 columns.\n",
    "\n",
    "Column Names and descriptions:\n",
    "* **id** - unique identified for a house\n",
    "* **dateDate** - house was sold\n",
    "* **pricePrice** -  is prediction target\n",
    "* **bedroomsNumber** -  of Bedrooms/House\n",
    "* **bathroomsNumber** -  of bathrooms/bedrooms\n",
    "* **sqft_livingsquare** -  footage of the home\n",
    "* **sqft_lotsquare** -  footage of the lot\n",
    "* **floorsTotal** -  floors (levels) in house\n",
    "* **waterfront** - House which has a view to a waterfront\n",
    "* **view** - Has been viewed\n",
    "* **condition** - How good the condition is ( Overall )\n",
    "* **grade** - overall grade given to the housing unit, based on King County grading system\n",
    "* **sqft_above** - square footage of house apart from basement\n",
    "* **sqft_basement** - square footage of the basement\n",
    "* **yr_built** - Built Year\n",
    "* **yr_renovated** - Year when house was renovated\n",
    "* **zipcode** - zip\n",
    "* **lat** - Latitude coordinate\n",
    "* **long** - Longitude coordinate\n",
    "* **sqft_living15** - The square footage of interior housing living space for the nearest 15 neighbors\n",
    "* **sqft_lot15** - The square footage of the land lots of the nearest 15 neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will import our custom functions from Mod1_Functions.py.\n",
    "\n",
    "We also import pandas, numpy, matplotlib, seaborn, statsmodels, and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.265665Z",
     "start_time": "2019-05-09T21:50:13.258821Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Mod1_Functions.py, line 306)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/anaconda3/envs/learn-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2961\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-01f0f20429c0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from Mod1_Functions import *\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/jon/code/flatiron_datascience/github/Module1_Project/Mod1_Functions.py\"\u001b[0;36m, line \u001b[0;32m306\u001b[0m\n\u001b[0;31m    return df_out\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from Mod1_Functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Raw Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the csv data into a pandas dataframe by using `pandas.read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.268357Z",
     "start_time": "2019-05-09T21:50:13.252Z"
    }
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows the count of any data columns that are missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.269992Z",
     "start_time": "2019-05-09T21:50:13.258Z"
    }
   },
   "outputs": [],
   "source": [
    "missing = df_raw.shape[0] - df_raw.count()\n",
    "print(missing[missing>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that `waterfront`, `view`, and `yr_renovated` are missing values.\n",
    "\n",
    "`date` is formatted as an object type, instead of a date (datetime64).\n",
    "\n",
    "`sqft_basement` is also formatted as an object type, when we would expect a number format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some scatter plots of each potential X variable compared to our target `'price'`.\n",
    "\n",
    "The custom function below accepts inputs of a dataframe and the name of the column that is the target.  Additional parameters can also be passed through to adjust the formatting.\n",
    "\n",
    "Warnings are printed for any variables that cannot immediately be graphed as numbers, due to their datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.271872Z",
     "start_time": "2019-05-09T21:50:13.263Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scatter_y(df_raw, 'price', ncols=2, figsize=(16,45))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.445117Z",
     "start_time": "2019-05-09T21:50:13.373779Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7d3ab9d57081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_raw' is not defined"
     ]
    }
   ],
   "source": [
    "df_raw.iloc[:, 2:].hist(figsize  = [16, 10])\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data using custom function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `clean_dataframe()` function takes in inputs of dataframe and a dictionary of adjustments.\n",
    "\n",
    "We first set our `data_adjustments` dictionary to contain the fields we want to change as keys and a list of adjustments as dictionary values.\n",
    "\n",
    "The parameters in the list are: \\[datatype, value to replace, value to replace with, replacement array\\]\n",
    "\n",
    "    - datatype: must be a valid data type\n",
    "    - value to replace: can be a single value string, integer, or np.nan\n",
    "    - value to replace with: can be a single value or can be a list with strings containg other column names in dataframe (see replacement array below)\n",
    "    - replacement array: contains a list of floats or integers, which are multiplied by the associated data field in the \"value to replace with\" list\n",
    "    \n",
    "The list items should be set to None for any parameters you do not wish to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found some issues with the following data fields and decided to make some adjustments.\n",
    "\n",
    "- `'date'` : Currently the date is formatted as a string and we would like to change it into a date (datetime64) so that we can use it in our model.\n",
    "\n",
    "\n",
    "- `'bedrooms'` : There was one unusually high value of 33 bedrooms for one record.  We decided to replace that value with 4, based on the properties nearest neighbors by `'price'` and `'sqft_living'`.\n",
    "\n",
    "\n",
    "- `'waterfront'`: There were 2,376 missing values in this field, which we have replaced with 'missing'.  We don't know whether these records are on the waterfront or not, but it is a significant enough proportion of records (11%), so we don't want to make an assumption about whether they are on the waterfront and also don't want to exclude them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.447565Z",
     "start_time": "2019-05-09T21:50:13.274Z"
    }
   },
   "outputs": [],
   "source": [
    "data_adjustments = {'date': ['datetime64', None, None, None],\n",
    "                    'bedrooms': [None, 33, 4, None],\n",
    "                    'waterfront': [str, np.nan, 'missing', None],\n",
    "                    'view': [str, np.nan, 0, None],\n",
    "                    'sqft_basement': [float, '?', ['sqft_living', 'sqft_above'], [1, -1]],\n",
    "                    'floors': [None, 3.5, 3, None]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.448907Z",
     "start_time": "2019-05-09T21:50:13.279Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean = clean_dataframe(df_raw, data_adjustments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features calculated from other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wondered whether there was any seasonality to home sale prices, so we needed to extract the month \n",
    "\n",
    "We created a `'month'` column in order to calculate a `'season'` column that will be used to categorize by season of the year.\n",
    "\n",
    "We also create a custom binned variable `'yr_renovated_cat'` which categorizes whether the house has been renovated and whether that renovation was recent.\n",
    "\n",
    "We have also add a dummy variable that classifies whether the house's square footage above ground is in the top 50% of the data set for square footage above ground.\n",
    "\n",
    "After some work further on with the model, we determined that there were 4 zip codes where the average sales price was much higher than in other zipcodes.  Rather than consider every zipcode as a dummy variable, we decided to make a single variable for whether or not the property is in one of the 4 high priced zipcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.449996Z",
     "start_time": "2019-05-09T21:50:13.284Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['month'] = df_clean['date'].map(lambda x: x.month)\n",
    "\n",
    "df_clean['season'] = df_clean['month'].apply(create_season)\n",
    "\n",
    "df_clean['has_basement'] = df_clean['sqft_basement'].apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "df_clean['sqft_above_tophalf'] = df_clean['sqft_above'].apply(lambda x: 1 if x>df_clean['sqft_above'].mean() else 0)\n",
    "\n",
    "df_clean['zip_highprice'] = df_clean['zipcode'].apply(lambda x: 1 if x in [98004, 98039, 98040, 98112] else 0)\n",
    "\n",
    "# Set number of years to consider recent renovation\n",
    "n_years = 5\n",
    "df_clean['yr_renovated_cat'] = df_clean['yr_renovated'].apply(\n",
    "    renovated_cat, n_years=n_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data fields as Categorical to avoid treating as numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use our `set_to_categorical()` function to take the list of variables shown below and convert each to a category datatype in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.451641Z",
     "start_time": "2019-05-09T21:50:13.288Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_columns = ['floors', 'waterfront', 'view', 'condition',\n",
    "                       'grade', 'yr_renovated_cat', 'season']\n",
    "# not in use: 'zipcode'\n",
    "set_to_categorical(df_clean, categorical_columns)\n",
    "\n",
    "print(\"Categorical Variables:\")\n",
    "print(df_clean.dtypes[df_clean.dtypes=='category'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have made a custom function `create_dummyframe()` which generates a data frame of dummy variables for each of the variables listed in the second parameter from the original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.453014Z",
     "start_time": "2019-05-09T21:50:13.292Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dummy = create_dummyframe(df_clean, categorical_columns)\n",
    "\n",
    "print('df_dummy shape: {}'.format(df_dummy.shape))\n",
    "print('\\nPreview:')\n",
    "df_dummy.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of prices.\n",
    "\n",
    "The prices appear to follow a log relationship.\n",
    "\n",
    "Let's consider evulating a log-linear relationship by adding a `'log_price'` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.454180Z",
     "start_time": "2019-05-09T21:50:13.296Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.price.hist(figsize  = [16, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.455435Z",
     "start_time": "2019-05-09T21:50:13.299Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean['log_price'] = np.log(df_clean['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And take another look at the scatter plots compared to the new target of `'log_price'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.456878Z",
     "start_time": "2019-05-09T21:50:13.302Z"
    }
   },
   "outputs": [],
   "source": [
    "scatter_y(df_clean, 'log_price', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.458378Z",
     "start_time": "2019-05-09T21:50:13.307Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.iloc[:, 2:].hist(figsize  = [16, 10])\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking another look at the distribution of prices after taking the log reveals that we have much closer to a normal distribution of log_price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.459911Z",
     "start_time": "2019-05-09T21:50:13.311Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(df_clean.log_price);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Correlation between our variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need evaluate the correlation matrix to determine whether any of our X variables are highly correlated, which would necessitate removing at least one of them to avoid multicolinearity problems.\n",
    "\n",
    "To make things easier, we have a function that will find any pairs of variables in the matrix with an absolute value of correlation greater than the second parameter (default = 0.75)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.461292Z",
     "start_time": "2019-05-09T21:50:13.316Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_pairs = findcorrpairs(df_clean, 0.5, )\n",
    "corr_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model\n",
    "\n",
    "## Initial Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our intuition about the scatter plots above, along with removing a few variables that would be correlated, we take our first shot at linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our list of initial X variables to try and model upon.\n",
    "\n",
    "Some intuitive features you would see in the listing for the home include the number of bedrooms and bathrooms and the square footage of the home and property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.462770Z",
     "start_time": "2019-05-09T21:50:13.320Z"
    }
   },
   "outputs": [],
   "source": [
    "x_list = [\"bedrooms\", \n",
    "          \"bathrooms\", \n",
    "          \"sqft_above\", \n",
    "          \"sqft_basement\", \n",
    "          \"sqft_lot\", \n",
    "          \"sqft_living15\", \n",
    "          \"sqft_lot15\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first run the regression using stats model's OLS function.  Note that the dataframe of X variables must include a column of constants (1).  We have decided to use `'log_price'` as our target (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.464483Z",
     "start_time": "2019-05-09T21:50:13.323Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_clean.loc[:, x_list]\n",
    "X = sm.add_constant(X)\n",
    "Y = df_clean['log_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below fits our model and sets it equal to the variable `model_init`.  We then take a look at the results with the `.summary()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.466066Z",
     "start_time": "2019-05-09T21:50:13.328Z"
    }
   },
   "outputs": [],
   "source": [
    "model_init = sm.OLS(Y, X).fit()\n",
    "model_init.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Not great...<br>\n",
    "    Our correlation of 0.516 isn't very good. <br><br> However, the silver lining is that the distribution of residuals appears to be approximately normally distributed based on the histogram of residuals and Q-Q plot below. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.467371Z",
     "start_time": "2019-05-09T21:50:13.331Z"
    }
   },
   "outputs": [],
   "source": [
    "resid1 = model_init.resid\n",
    "resid1.hist()\n",
    "fig = sm.graphics.qqplot(resid1, dist=stats.norm, line='45', fit=True, alpha=0.05)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our model a little smarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the scatterplot of possible x variables and `'log_price'` we assess that the following variables should have the log function applied in order for their relationships with `'log_price'` to be linear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.468947Z",
     "start_time": "2019-05-09T21:50:13.335Z"
    }
   },
   "outputs": [],
   "source": [
    "log_list = ['sqft_above', 'sqft_basement',\n",
    "            'sqft_living15', 'sqft_lot', 'sqft_lot15']\n",
    "\n",
    "# avoid value of 0 in sqft_basement\n",
    "df_clean['sqft_basement'] = df_clean['sqft_basement'] + 1\n",
    "\n",
    "for i in log_list:\n",
    "    try:\n",
    "        logcount\n",
    "    except:\n",
    "        logcount = 1\n",
    "        df_clean[i] = df_clean[i].apply(np.log)   #apply(lambda x: np.log(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to convert our date field into a number of days from an arbitrary starting point so that we can treat each day as a unit.  For lack of a better date, we start from 1/1/1970.  This date does not matter because we will normalize the variable later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.470564Z",
     "start_time": "2019-05-09T21:50:13.338Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean['startdate'] = pd.Timestamp('19700101')\n",
    "\n",
    "df_clean['date_num'] = (df_clean['date'] - df_clean['startdate']).dt.days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that there were some very large outliers in the fields of `'sqft_lot'` and `'sqft_lot15'`.\n",
    "\n",
    "Our condition below removes any outliers beyond the 99.9th percentile.\n",
    "\n",
    "The output below the code shows that 38 total outliers were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.471889Z",
     "start_time": "2019-05-09T21:50:13.342Z"
    }
   },
   "outputs": [],
   "source": [
    "f1_var = 'sqft_lot'\n",
    "f2_var = 'sqft_lot15'\n",
    "\n",
    "f1_lim = df_clean[f1_var].quantile(.999)\n",
    "f2_lim = df_clean[f2_var].quantile(.999)\n",
    "\n",
    "n1 = len(df_clean[df_clean[f1_var]>=f1_lim])\n",
    "n2 = len(df_clean[df_clean[f2_var]>=f2_lim])\n",
    "\n",
    "filter1 = df_clean[f1_var]<f1_lim\n",
    "filter2 = df_clean[f2_var]<f2_lim\n",
    "\n",
    "print('filtered out {} records with {} greater than: {}'.format(n1, f1_var, f1_lim))\n",
    "print('filtered out {} records with {} greater than: {}'.format(n2, f2_var, f2_lim))\n",
    "\n",
    "df_filter = df_clean[ filter1 & filter2 ] \n",
    "\n",
    "print('{} total records removed'.format(len(df_filter) - len(df_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our variables now contain a different number of rows, we need to recreate the dummy variable dataframe.\n",
    "\n",
    "This can be achieved by using our `create_dummyframe()` function again, but using the `df_filter` as an input instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.473250Z",
     "start_time": "2019-05-09T21:50:13.346Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dummy_filter = create_dummyframe(df_filter, categorical_columns)\n",
    "\n",
    "print('df_dummy shape: {}'.format(df_dummy_filter.shape))\n",
    "print('\\nPreview:')\n",
    "df_dummy_filter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling our Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure no variables have any added or diminished effect simply due to the magnitude of the variable, we will standardize.\n",
    "\n",
    "Some variables are standardized using the `MinMaxScaler()`, while others use the `StandardScaler()`.\n",
    "\n",
    "We defaulted to using the `StandardScaler()` except for those variables that can be represented well by a specific domain.  \n",
    "\n",
    "In this case, we thought that was appropriate for the `'date_num'` and `'yr_built'` data fields because they can be thought of as positions of dates within a specific range of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.474468Z",
     "start_time": "2019-05-09T21:50:13.350Z"
    }
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "\n",
    "Y = df_filter['log_price']\n",
    "\n",
    "\n",
    "unadj_list = ['has_basement', 'zip_highprice']\n",
    "\n",
    "  \n",
    "min_max_list = ['date_num', 'yr_built']\n",
    "\n",
    "std_scal_list = ['sqft_lot', 'sqft_above', \n",
    "                 'sqft_living15', 'sqft_lot15',]\n",
    "\n",
    "X_unadj = pd.DataFrame(columns=unadj_list)\n",
    "X_min_max = pd.DataFrame(columns=min_max_list)\n",
    "X_std_scal = pd.DataFrame(columns=std_scal_list)\n",
    "\n",
    "\n",
    "\n",
    "for col_name in min_max_list:\n",
    "    X_min_max[col_name] = df_filter[col_name]\n",
    "    \n",
    "for col_name in std_scal_list:\n",
    "    X_std_scal[col_name] = df_filter[col_name]\n",
    "\n",
    "for col_name in unadj_list:\n",
    "    X_unadj[col_name] = df_filter[col_name]   \n",
    "\n",
    "\n",
    "X_min_max = pd.DataFrame(data= min_max_scaler.fit_transform(X_min_max.values), columns=X_min_max.columns)\n",
    "\n",
    "X_std_scal = pd.DataFrame(data= standard_scaler.fit_transform(X_std_scal.values), columns=X_std_scal.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to filtering, some of our indices became mismatched.  However, our rows still line up, so we reset the axes to concatenate all of our various X dataframes together into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.476050Z",
     "start_time": "2019-05-09T21:50:13.354Z"
    }
   },
   "outputs": [],
   "source": [
    "X_min_max.reset_index(inplace=True)\n",
    "X_std_scal.reset_index(inplace=True)\n",
    "X_unadj.reset_index(inplace=True) \n",
    "df_dummy_filter.reset_index(inplace=True)\n",
    "Y = Y.reset_index().drop(['index'], axis=1)\n",
    "\n",
    "\n",
    "X_possible = pd.concat([X_min_max, X_std_scal, X_unadj, df_dummy_filter], axis=1)\n",
    "X_possible.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove several columns.  Notably, each of our categorical column groups needs at least one removed to avoid multicolinearity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.477299Z",
     "start_time": "2019-05-09T21:50:13.358Z"
    }
   },
   "outputs": [],
   "source": [
    "X_model = X_possible.drop(['floors_1.0', 'waterfront_missing', 'view_0.0', 'condition_1',\n",
    "                           'grade_3', 'yr_renovated_cat_Never Renovated', \n",
    "                           'season_Winter', 'index', 'waterfront_1.0', 'yr_renovated_cat_missing',\n",
    "                          'grade_6', 'waterfront_0.0', 'condition_2'], axis=1)\n",
    "# 'zipcode_98001',\n",
    "X_model = sm.add_constant(X_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.478366Z",
     "start_time": "2019-05-09T21:50:13.362Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_smarter = sm.OLS(Y, X_model).fit()\n",
    "model_smarter.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.479529Z",
     "start_time": "2019-05-09T21:50:13.365Z"
    }
   },
   "outputs": [],
   "source": [
    "X_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.480733Z",
     "start_time": "2019-05-09T21:50:13.369Z"
    }
   },
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "\n",
    "linreg.fit(X_model, Y)\n",
    "\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.481899Z",
     "start_time": "2019-05-09T21:50:13.372Z"
    }
   },
   "outputs": [],
   "source": [
    "model_kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "MSEs = cross_val_score(linreg, X_model, Y, scoring='neg_mean_squared_error', cv=model_kfold)\n",
    "\n",
    "mean_MSE = np.mean(MSEs)\n",
    "\n",
    "print(mean_MSE)\n",
    "print(MSEs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing different numbers of variables using Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.483134Z",
     "start_time": "2019-05-09T21:50:13.380Z"
    }
   },
   "outputs": [],
   "source": [
    "cycles = 10\n",
    "\n",
    "n = len(X_model.columns)\n",
    "\n",
    "mse_list = []\n",
    "r2_list = []\n",
    "num_var = list(range(1, n+1))\n",
    "\n",
    "    \n",
    "for i in range(n):\n",
    "    selector = RFE(linreg, n_features_to_select = i+1)\n",
    "    selector = selector.fit(X_model, Y['log_price'])\n",
    "    selected_X = X_model.columns[selector.support_]\n",
    "    df_selected = X_model.loc[:, selected_X]\n",
    "\n",
    "    model_kfold = KFold(n_splits=10, shuffle=True)\n",
    "    MSEs = cross_val_score(linreg, df_selected , Y, scoring='neg_mean_squared_error', cv=model_kfold)\n",
    "    mean_MSE = np.mean(MSEs)\n",
    "    model = sm.OLS(Y, df_selected).fit()\n",
    "    \n",
    "    r2_list.append(model.rsquared_adj)\n",
    "    mse_list.append(mean_MSE)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(r2_list, label='r squared')\n",
    "plt.xlabel(r\"Description of $x$ coordinate (units)\")\n",
    "plt.ylabel(r\"Description of $y$ coordinate (units)\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.plot(mse_list, label='mse')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of Final Model Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.484276Z",
     "start_time": "2019-05-09T21:50:13.386Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selector = RFE(linreg, n_features_to_select = 34)\n",
    "selector = selector.fit(X_model, Y['log_price'])\n",
    "selected_X = X_model.columns[selector.support_]\n",
    "df_selected = X_model.loc[:, selected_X]\n",
    "\n",
    "model = sm.OLS(Y, df_selected).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.485951Z",
     "start_time": "2019-05-09T21:50:13.391Z"
    }
   },
   "outputs": [],
   "source": [
    "resid1 = model.resid\n",
    "resid1.hist()\n",
    "# resid1[resid1 > 1.75]\n",
    "# plt.scatter(Y, resid1)\n",
    "fig = sm.graphics.qqplot(resid1, dist=stats.norm, line='45', fit=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:50:13.487893Z",
     "start_time": "2019-05-09T21:50:13.395Z"
    }
   },
   "outputs": [],
   "source": [
    "model_kfold = KFold(n_splits=10, shuffle=True)\n",
    "MSEs = cross_val_score(linreg, df_selected , Y, scoring='neg_mean_squared_error', cv=model_kfold)\n",
    "MSEs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "155px",
    "width": "389px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "296px",
    "left": "1127px",
    "top": "110px",
    "width": "260.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "454.016px",
    "left": "486.125px",
    "right": "20px",
    "top": "109.969px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
